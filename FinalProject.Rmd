---
title: "Machine Learning Course Final Project"
author: "MÃ³nica Figueroa"
date: "November 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction.  

This project used data from a study related to activity recognition (Velloso et al., 2013). The aim of the study was to predict categories of performance of a weight-lifting exercise (Unilateral Dumbbell Biceps Curl) using data generated by sensors placed in specific parts of the body of six male participants. The data of this study can be retrived in [Human Activity Recognition Webpage](http://groupware.les.inf.puc-rio.br/har#dataset). For this project two data sets were used:  
*  A training data set (https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)  
*  A test data set (https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)  

## Data pre-processing.  

A first look into the training data lead to:

1. Substitute the string "#DIV70!"  and blank spaces to NA.  
2. Eliminate variables with a majority of NA values (more than 30%).  
3. Eliminate the first variable (number of observation)
4. Eliminate the second variable (name of subject)  
5. Eliminate raw_timestamp_part_1 and raw_timestamp_part_1 since these information is in cvtd_timestamp
6. Eliminate window variables  

```{r}
DATOS<-read.csv("pml-training.csv", na.strings = c("#DIV/0!","NA", ""), stringsAsFactors = FALSE)
PROP_NA <- apply(DATOS, 2, function(x) sum(is.na(x)))/nrow(DATOS) # Proportion of NAs per variable
HIGH_PROP_NA<-PROP_NA > 0.3 # Identify those variables with high amount of NAs (more than 30%)
DATOS2<-DATOS[, !HIGH_PROP_NA] # Eliminate variables with a high proportion of NAs
DATOS2<-DATOS2[, -(1:7)]# Eliminate first seven variables

# Same treatment to de test data
TEST<-read.csv("pml-testing.csv", na.strings = c("#DIV/0!","NA", ""), stringsAsFactors = FALSE)
PROP_NA_TEST <- apply(TEST, 2, function(x) sum(is.na(x)))/nrow(TEST)
HIGH_PROP_NA_TEST <- PROP_NA_TEST > 0.3
TEST<-TEST[, !HIGH_PROP_NA_TEST]
TEST<-TEST[, -(1:7)]

# Partition of the procesed training data set
library(caret)
set.seed(1234)
inTrain<-createDataPartition(DATOS2$classe, p = 0.75)[[1]]
training<-DATOS2[inTrain,]
testing<-DATOS2[-inTrain,]


```

## Fiting a model.  

A random forest model was build with the training data set. I selected this model because despite the fact that fiftythree variables are too many variables none of them by themselves seem to have classificatory power (analysis not shown). Even using principal components the information is not capture in few components. The graph above shows how its no posible to classify correctly with the first two components which are the onse with more information in relation to the resto of the components.    

```{r}
compos<-princomp(training[,-53])
CP1<-compos$scores[,1]
CP2<-compos$scores[,2]
colores<-as.factor(training$classe)
plot(CP1, CP2, pch=20, col = c("red", "blue", "green", "yellow", "purple")[colores])

set.seed(5678)
mod1<-train(classe ~ . , method = "rf", data = training, ntree = 10)
mod1

```


## Testing the model

Testing the model with the testing set (not the 20 testing set) gave a high accuracy.

```{r}
confusionMatrix(testing$classe, predict(mod1, testing))

```

## Conclusion.

The random forest model was efficient in detecting the different kinds of performance categories. The sensor generated data, although is noisy, together work well to identify the most common mistakes made at the Unilateral Dumbbell Biceps Curl excersise.

## References

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

